

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Guided Policy Learning &mdash; VISTA Simulator  documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/favicon.png"/>
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Reinforcement Learning" href="reinforcement_learning.html" />
    <link rel="prev" title="Imitation Learning" href="imitation_learning.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> VISTA Simulator
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../introduction/index.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/index.html">Getting Started</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Advanced Usage</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="imitation_learning.html">Imitation Learning</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Guided Policy Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="reinforcement_learning.html">Reinforcement Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="testing_with_augmented_reality.html">Testing With Augmented Reality</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../interface_to_public_dataset/index.html">Interface To Public Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_documentation/index.html">API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/index.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../acknowledgement/index.html">Acknowledgement</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">VISTA Simulator</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="index.html">Advanced Usage</a> &raquo;</li>
        
      <li>Guided Policy Learning</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/advanced_usage/guided_policy_learning.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="guided-policy-learning">
<span id="advanced-usage-guided-policy-learning"></span><h1>Guided Policy Learning<a class="headerlink" href="#guided-policy-learning" title="Permalink to this headline">¶</a></h1>
<p>Here we demonstrate how to leverage the power of local synthesis around a passive dataset using
Vista, which leads to a learning framework called guided policy learning (GPL). In contrast to
imitation learning (IL), GPL actively samples sensor data that is around but different from the original
dataset and couple it with control commands (labels in supervised imitation learning) that aims to
correct the agent to the nominal human trajectory in the original passive dataset. Overall, GPL can
be seen as a data augmentation version of IL that tries to improve robustness of the model within
some deviation of demonstration (the passive dataset).</p>
<p>Similar to IL, we first initialize Vista simulator and a sampler. There are two major difference
during reset. First, we always initialize the ego agent some distance away from the human trajectory
(demonstration) to actively scenarios to be corrected from. This is specified by <code class="docutils literal notranslate"><span class="pre">initial_dynamics_fn</span></code>.
Second, we need a controller that provides ground truth control commands to correct toward the
demonstration. The controller is allowed to have access to privileged information (e.g., lane
boundaries, ado cars’ poses, etc) as it is only used to provide guidance for the policy learning
during training time.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">_world</span> <span class="o">=</span> <span class="n">vista</span><span class="o">.</span><span class="n">World</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">trace_paths</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">trace_config</span><span class="p">)</span>
<span class="bp">self</span><span class="o">.</span><span class="n">_agent</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_world</span><span class="o">.</span><span class="n">spawn_agent</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">car_config</span><span class="p">)</span>
<span class="bp">self</span><span class="o">.</span><span class="n">_camera</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_agent</span><span class="o">.</span><span class="n">spawn_camera</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">camera_config</span><span class="p">)</span>
<span class="bp">self</span><span class="o">.</span><span class="n">_world</span><span class="o">.</span><span class="n">reset</span><span class="p">({</span><span class="bp">self</span><span class="o">.</span><span class="n">_agent</span><span class="o">.</span><span class="n">id</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">initial_dynamics_fn</span><span class="p">})</span>
<span class="bp">self</span><span class="o">.</span><span class="n">_sampler</span> <span class="o">=</span> <span class="n">RejectionSampler</span><span class="p">()</span>

<span class="bp">self</span><span class="o">.</span><span class="n">_privileged_controller</span> <span class="o">=</span> <span class="n">get_controller</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">privileged_control_config</span><span class="p">)</span>
</pre></div>
</div>
<p>Next, we implement a data generator that produce a training dataset “around” the original passive
dataset used for imitation learning that encourages the policy to correct itself toward the
demonstration (human trajectories).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Data generator from simulation</span>
<span class="bp">self</span><span class="o">.</span><span class="n">_snippet_i</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="c1"># reset simulator</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_agent</span><span class="o">.</span><span class="n">done</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_snippet_i</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">snippet_size</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_world</span><span class="o">.</span><span class="n">reset</span><span class="p">({</span><span class="bp">self</span><span class="o">.</span><span class="n">_agent</span><span class="o">.</span><span class="n">id</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">initial_dynamics_fn</span><span class="p">})</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_snippet_i</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># privileged control</span>
    <span class="n">curvature</span><span class="p">,</span> <span class="n">speed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_privileged_controller</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_agent</span><span class="p">)</span>

    <span class="c1"># step simulator</span>
    <span class="n">sensor_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_camera</span><span class="o">.</span><span class="n">name</span>
    <span class="n">img</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_agent</span><span class="o">.</span><span class="n">observations</span><span class="p">[</span><span class="n">sensor_name</span><span class="p">]</span> <span class="c1"># associate action t with observation t-1</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">curvature</span><span class="p">,</span> <span class="n">speed</span><span class="p">])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_agent</span><span class="o">.</span><span class="n">step_dynamics</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

    <span class="n">val</span> <span class="o">=</span> <span class="n">curvature</span>
    <span class="n">sampling_prob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_sampler</span><span class="o">.</span><span class="n">get_sampling_probability</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">sampling_prob</span><span class="p">:</span> <span class="c1"># reject</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_snippet_i</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">continue</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_sampler</span><span class="o">.</span><span class="n">add_to_history</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>

    <span class="c1"># preprocess and produce data-label pairs</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">transform_rgb</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_camera</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">train</span><span class="p">)</span>
    <span class="n">label</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">curvature</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_snippet_i</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">yield</span> <span class="p">{</span><span class="s1">&#39;camera&#39;</span><span class="p">:</span> <span class="n">img</span><span class="p">,</span> <span class="s1">&#39;target&#39;</span><span class="p">:</span> <span class="n">label</span><span class="p">}</span>
</pre></div>
</div>
<p>As shown above, the only difference from imitation learning is to run a privileged controller that
produces the correct control commands for states deviated from the human trajectories. Thus, we get
data of agent initially deviating away from the demonstration but gradually converging to the human
trajectories. At test time with closed-loop control setting, such training scheme allows the policy
to correct itself from drifting away due to compounding error. For more details, please check
<code class="docutils literal notranslate"><span class="pre">examples/advanced_usage/gpl_rgb_dataset.py</span></code>.</p>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="reinforcement_learning.html" class="btn btn-neutral float-right" title="Reinforcement Learning" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="imitation_learning.html" class="btn btn-neutral float-left" title="Imitation Learning" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2022, VISTA

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>