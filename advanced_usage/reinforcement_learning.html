

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Reinforcement Learning &mdash; VISTA Simulator  documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/favicon.png"/>
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Testing With Augmented Reality" href="testing_with_augmented_reality.html" />
    <link rel="prev" title="Guided Policy Learning" href="guided_policy_learning.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> VISTA Simulator
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../introduction/index.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/index.html">Getting Started</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Advanced Usage</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="imitation_learning.html">Imitation Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="guided_policy_learning.html">Guided Policy Learning</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Reinforcement Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="testing_with_augmented_reality.html">Testing With Augmented Reality</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../interface_to_public_dataset/index.html">Interface To Public Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_documentation/index.html">API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/index.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../acknowledgement/index.html">Acknowledgement</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">VISTA Simulator</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="index.html">Advanced Usage</a> &raquo;</li>
        
      <li>Reinforcement Learning</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/advanced_usage/reinforcement_learning.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="reinforcement-learning">
<span id="advanced-usage-reinforcement-learning"></span><h1>Reinforcement Learning<a class="headerlink" href="#reinforcement-learning" title="Permalink to this headline">¶</a></h1>
<p>Based on what we discussed in the previous section for guided policy learning where we actively
generate data with off-human-trajectory initialization and privileged controller for correction,
we can consider the further extreme of exploiting the “activeness” of applying Vista on a passive
dataset. This leads to reinforcement learning (RL), where an agent is allowed to interact with the
environment, collect training data itself, and learn to achieve good task performance. Here we show
how to use Vista to define a RL environment for learning a policy.</p>
<p>Implementation-wise, there are four major components to be defined, namely observations, environment
dynamics, reward function, and terminal condition. Observations are simply sensor data and environment
dynamics is how the ego-car move after applying some control commands, which is already embedded in
vehicle states update (and collision if multi-agent scenario is considered). Thus, we need to further
define reward function and terminal condition. For example, if we consider lane-following,</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">dt</span><span class="o">=</span><span class="mi">1</span> <span class="o">/</span> <span class="mf">30.</span><span class="p">):</span>
    <span class="c1"># Step agent and get observation</span>
    <span class="n">agent</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_world</span><span class="o">.</span><span class="n">agents</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">action</span><span class="p">[</span><span class="n">agent</span><span class="o">.</span><span class="n">id</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">agent</span><span class="o">.</span><span class="n">human_speed</span><span class="p">])</span>
    <span class="n">agent</span><span class="o">.</span><span class="n">step_dynamics</span><span class="p">(</span><span class="n">action</span><span class="p">,</span> <span class="n">dt</span><span class="o">=</span><span class="n">dt</span><span class="p">)</span>
    <span class="n">agent</span><span class="o">.</span><span class="n">step_sensors</span><span class="p">()</span>
    <span class="n">observations</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">observations</span>

    <span class="c1"># Define terminal condition</span>
    <span class="n">done</span><span class="p">,</span> <span class="n">info_from_terminal_condition</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;terminal_condition&#39;</span><span class="p">](</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">agent</span><span class="o">.</span><span class="n">id</span><span class="p">)</span>

    <span class="c1"># Define reward</span>
    <span class="n">reward</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;reward_fn&#39;</span><span class="p">](</span><span class="bp">self</span><span class="p">,</span> <span class="n">agent</span><span class="o">.</span><span class="n">id</span><span class="p">,</span>
                                         <span class="o">**</span><span class="n">info_from_terminal_condition</span><span class="p">)</span>

    <span class="c1"># Get info</span>
    <span class="c1"># ...</span>

    <span class="c1"># Pack output</span>
    <span class="n">observations</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_append_agent_id</span><span class="p">,</span> <span class="p">[</span><span class="n">observations</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">observations</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span>
</pre></div>
</div>
<p>, where <code class="docutils literal notranslate"><span class="pre">self.config['terminal_condition']</span></code> can be defined as when the car is off the lane (too
far away from the lane center) or car heading deviate too much from road curvature. Note that apart
from being the terminal condition for lane following task, the above-mentioned two constraints
should be satisfied since Vista only allow high-fidelity synthesis locally around the original
passive dataset.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">default_terminal_condition</span><span class="p">(</span><span class="n">task</span><span class="p">,</span> <span class="n">agent_id</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">agent</span> <span class="o">=</span> <span class="p">[</span><span class="n">_a</span> <span class="k">for</span> <span class="n">_a</span> <span class="ow">in</span> <span class="n">task</span><span class="o">.</span><span class="n">world</span><span class="o">.</span><span class="n">agents</span> <span class="k">if</span> <span class="n">_a</span><span class="o">.</span><span class="n">id</span> <span class="o">==</span> <span class="n">agent_id</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">_check_out_of_lane</span><span class="p">():</span>
        <span class="n">road_half_width</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">trace</span><span class="o">.</span><span class="n">road_width</span> <span class="o">/</span> <span class="mf">2.</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">relative_state</span><span class="o">.</span><span class="n">x</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">road_half_width</span>

    <span class="k">def</span> <span class="nf">_check_exceed_max_rot</span><span class="p">():</span>
        <span class="n">maximal_rotation</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="mf">10.</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">relative_state</span><span class="o">.</span><span class="n">theta</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">maximal_rotation</span>

    <span class="n">out_of_lane</span> <span class="o">=</span> <span class="n">_check_out_of_lane</span><span class="p">()</span>
    <span class="n">exceed_max_rot</span> <span class="o">=</span> <span class="n">_check_exceed_max_rot</span><span class="p">()</span>
    <span class="n">done</span> <span class="o">=</span> <span class="n">out_of_lane</span> <span class="ow">or</span> <span class="n">exceed_max_rot</span> <span class="ow">or</span> <span class="n">agent</span><span class="o">.</span><span class="n">done</span>
    <span class="n">other_info</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;done&#39;</span><span class="p">:</span> <span class="n">done</span><span class="p">,</span>
        <span class="s1">&#39;out_of_lane&#39;</span><span class="p">:</span> <span class="n">out_of_lane</span><span class="p">,</span>
        <span class="s1">&#39;exceed_max_rot&#39;</span><span class="p">:</span> <span class="n">exceed_max_rot</span><span class="p">,</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="n">done</span><span class="p">,</span> <span class="n">other_info</span>
</pre></div>
</div>
<p>We can define a very simple reward function that encourage survival (not going off the lane or
exceeding some rotation with respect to the road curvature) by simply checking whether the current
step is terminated.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">default_reward_fn</span><span class="p">(</span><span class="n">task</span><span class="p">,</span> <span class="n">agent_id</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; An example definition of reward function. &quot;&quot;&quot;</span>
    <span class="n">reward</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="k">if</span> <span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;done&#39;</span><span class="p">]</span> <span class="k">else</span> <span class="mi">0</span>  <span class="c1"># simply encourage survival</span>

    <span class="k">return</span> <span class="n">reward</span><span class="p">,</span> <span class="p">{}</span>
</pre></div>
</div>
<p>Please check <a class="reference internal" href="../api_documentation/tasks.html#api-lane-following"><span class="std std-ref">lane_following.py</span></a> for more details. The implementation
rougly follow <a class="reference external" href="https://gym.openai.com/">OpenAI Gym</a> interface with <code class="docutils literal notranslate"><span class="pre">reset</span></code> and <code class="docutils literal notranslate"><span class="pre">step</span></code> functions.
However, there are still other attributes or functions to be implemented like <code class="docutils literal notranslate"><span class="pre">action_space</span></code>,
<code class="docutils literal notranslate"><span class="pre">observation_space</span></code>, <code class="docutils literal notranslate"><span class="pre">render</span></code>, etc, which may require objects from <code class="docutils literal notranslate"><span class="pre">gym</span></code>.</p>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="testing_with_augmented_reality.html" class="btn btn-neutral float-right" title="Testing With Augmented Reality" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="guided_policy_learning.html" class="btn btn-neutral float-left" title="Guided Policy Learning" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2022, VISTA

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>